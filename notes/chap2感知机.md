# 第二章 感知机

+ 感知机(perceptron)是二类分类的线性分类模型，输入为实例的特征向量，输出为实例的类别。
+ 感知机属于**判别模型**
+ 感知机主要是为了求出将训练数据进行线性划分的分离超平面，导入误分类的损失函数，利用**梯度下降法**对损失函数进行极小化，求得感知机模型

## 2.1 感知机模型

**定义 2.1**(感知机)假设输入空间（特征空间）是 $X \in R^n$，输出空间是 $ Y={+1,-1}$ . 输入 $x \in X$ 表示实例的特征向量，输出 $y \in Y$. 则由输入到输出空间的如下函数

$$ f(x)=sign(w \cdot x +b)$$

其中 $w\in R^n$ 叫做权值(weight)或权值向量(weight vector)，$b \in R$ 叫做偏置(bias), $sign$ 是符号函数

$$sign(x)=
\begin{cases}
+1& x \geq 0 \\
-1& x < 0
\end{cases}$$

感知机可以如下解释，由 $w$ 和 $b$ 得到一个对应于特征空间 $R^n$ 的超平面 $S$ , 如下式所示

$$w \cdot x + b = 0$$

这个超平面将空间划分为两个部分，位于两部分的点(特征向量)分别分为正、负两类. 因此，超平面 $S$ 称为分离超平面(separating hyperplane)

## 2.2 感知机学习策略

### 2.2.1 数据集的线性可分性

**定义 2.2** (数据集的线性可分性) 给定一个数据集

$$T={(x_1, y_1), (x_2, y_2),...,(x_N,y_N)}$$

如果存在超平面 $S$ 能够将数据集的正实例点和负实例点完全正确地划分到超平面的两侧，则称数据集 $T$ 为线性可分数据集(linear separable data set)；否则，称数据集 $T$ 线性不可分

## 2.2.2 感知机的学习策略

>在训练数据集线性可分的前提假设下，感知机的目标是找到分离超平面，一般的策略是定义(经验)损失函数并将损失函数极小化

感知机 $sign(w \cdot x + b)$学习的损失函数定义为

$$L(w, b)=-\sum \limits_{x_i \in M}y_i(w \cdot x_i + b)$$

这个损失函数就是感知机学习的经验风险函数

感知机的学习策略就是在假设空间中选取是损失函数最小的模型参数 $w, b$，即感知模型.

## 2.3 感知机学习算法

感知机学习算法分为两大类：

+ 原始形式
+ 对偶形式

### 2.3.1 感知机学习算法的原始形式

求解损失函数极小化情况下，参数$w,b$的值

$$ \min \limits_{w,b} L(w,b)=-\sum \limits_{x_i \in M} y_i(w \cdot x_i + b) $$

采用随机梯度下降法，求解的算法描述如下

**算法 2.1** (感知机学习算法的原始形式)

输入：训练数据集$T={(x_1, y_1), (x_2, y_2),...,(x_N,y_N)}$，其中$x_i \in X=R^N$, $y_i \in Y = \{-1, +1\}$, $i=1,2,...,N$；学习率$\eta(0< \eta \leq 1)$

输出：$w, b$; 感知机模型 $f(x)=sign(w \cdot x + b)$

(1) 选取初始值 $w_0, b_0$
(2) 在训练集中选取数据(x_i, y_i)
(3) 如果$y_i(w \cdot x_i + b) \leq 0$

$$\begin{array}{cc}
w \leftarrow w + \eta y_i x_i\\b \leftarrow b + \eta y_i
\end{array}$$

(4) 转至(2)，直至训练集中没有误分类点

## 2.3.2 算法的收敛性

> 对于线性可分数据感知集学习算法原始形式收敛，即经过有限次迭代可以得到一个将训练数据集完全正确划分的分离超平面及感知机模型

**定理 2.1** （Novikoff) 设训练数据集$T={(x_1, y_1), (x_2, y_2),...,(x_N,y_N)}$是线性可分的，其中$x_i\in X=R^n$, $y_i\in Y={-1, +1}$, $i=1,2,...,N$，则

(1) 存在满足条件$||\hat{w}_{opt}||=1$的超平面 $\hat{w}_{opt} \cdot \hat{x} = w_{opt} \cdot x + b_{opt} = 0$将训练数据集完全正确分开；且存在$\gamma > 0$，对所有的$i=1,2,...,N$

$$ y_i(\hat{w}_{opt} \cdot \hat{x}_i)=y_i(w_{opt} \cdot x_i + b_{opt} \geq \gamma$$

(2) 令 $R=\max \limits_{1 \leq i \geq N}||\hat{x}_i||$，则感知机算法在训练数据集上的误分类次数 $k$ 满足不等式

$$k \leq (\frac{R}{\gamma})^2$$

+ 定理表明，误分类的次数$k$是有上界的，经过有限次搜索可以找到将训练数据完全正确分开的分离超平面
+ 当训练集线性不可分时，感知机学习算法不收敛，迭代结果会发生震荡

### 2.3.3 感知机学习算法的对偶形式

**算法2.2**（感知机学习算法的对偶形式）

输入：线性可分的训练数据集$T={(x_1, y_1), (x_2, y_2),...,(x_N,y_N)}$，其中$x_i \in X=R^N$, $y_i \in Y = \{-1, +1\}$, $i=1,2,...,N$；学习率$\eta(0< \eta \leq 1)$

输出：$a,b$; 感知机模型$f(x)=sign(\sum \limits_{j=1}^N a_j y_j x_j \cdot x + b)$.
其中$a=(a_1,a_2,...,a_N)^T$.

(1) $a \leftarrow 0, b\leftarrow 0$
(2) 在训练集中选取数据$(x_i,y_i)$
(3) 如果$y_i(\sum \limits_{j=1}^N a_j y_j x_j \cdot x + b)\leq 0$

$$\begin{array}{cc}
a_i \leftarrow a_i + \eta\\
b \leftarrow b + \eta y_i
\end{array}$$

(4) 转至(2)直到没有误分类数据

>与原始形式一样，感知机学习算法的对偶形式迭代是收敛的，存在多个解
