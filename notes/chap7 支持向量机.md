# 第七章 支持向量机(Support Vector Machines, SVM)

## 前言

支持向量机这部分的知识点断断续续看了一周，看的头疼，至今仍有许多疑惑。在理解透彻之前先记下部分总结，也包括一些不懂的点，整理一下看的知识点，等有时间再回过头来仔细看看。

支持向量机(*Support Vector Machine*, SVM)的大名想必大家早有耳闻，其功能强大且用途广泛，既可以进行线性分类也可以进行非线性分类，甚至还可以进行异常值检测。SVM最大的优点是适用于小或中等样本量的复杂数据集。

作为定义在特征空间上的线性分类器，SVM有别于感知机的一点是它采用了间隔最大化的一个学习策略，可以形式化为一个求解凸二次规划(convec quadratic programming)的问题。

SVM从简单到复杂分为：

* 线性可分支持向量机(linear support vector machine in linearly separable case)
* 线性支持向量机(linear support vector machine)
* 非线性支持向量机(non-linear support vector machine)。

简单的模型是复杂模型的基础，而复杂模型是简单模型的特殊情况，类似于马哲中个性与共性的关系。三种模型的区分如下：

* 训练数据可分，通过硬间隔最大化(hard margin maximization)，得到一个线性分类器，即线性可分支持向量机，也称硬间隔支持向量机
* 训练数据接近线性可分时，通过软间隔最大化(soft margin maximization)，得到的线性分类器，即线性支持向量机，又称为软间隔支持向量机
* 训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机

以下按照上述所述的顺序介绍三种SVM、核函数以及SVM快速学习方法——序列最小最优化算法(SMO)

## 7.1 线性可分支持向量机与硬间隔最大化

### 7.1.1 线性可分支持向量机

首先给出线性可分支持向量机的训练数据描述，假设给定一个特征空间上的训练数据集

$$T={(x_1,y_1), (x_2, y_2),…,(x_N,y_N)}$$

其中，$x_i\in X = R^n$，即 $x_i = (x_{11}, x_{12}, …, x_{1m})^T$，$y_i \in Y = \{+1, -1\}，i=(1,2,…,N)$，$x_i$ 为第 $i$ 个特征向量，被称为实例，每个实例的维数为$m$，即特征维度为 $m$ 维，$y_i$ 是 $x_i$ 的标记，当 $y_i = +1$ 时，称 $x_i$ 为正例；当 $y_i=-1$ 时，称 $x_i$ 为负例，假设此时训练数据集是线性可分的。

分类学习最基本的思想是在样本空间找到一个划分超平面，将不同的类别的样本划分开来。线性可分支持向量机利用**间隔最大化**求最优分离超平面，这时，解是**唯一**的。

如下图所示，左图为普通的分割，其中除了绿色点线，都正确地分类，但是分割平面离实例点很近，如果训练数据有特异值或者误差，就不能很好地预测。右图是采用间隔最大化策略分割的SVM分类器分割的结果，可以看到这条分割线不仅将两类分隔开来而且离最近的训练实例尽可能的远。
![分离超平面](https://img-blog.csdnimg.cn/20200812141718900.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MTc4OTYy,size_16,color_FFFFFF,t_70#pic_center)

这样我们给出了线性可分支持向量机的一个定义

**定义 7.1 (线性可分支持向量机)** 给定**线性可分**训练数据集，通过**间隔最大化**或等价地求解相应的凸二次规划问题学习得到的分离超平面为

$$w^*x+b^*=0$$

以及相应的分类决策函数

$$f(x)=sign(w^*x+b^*)$$

称为线性可分支持向量机

其中超平面的参数 $w=(w_1, w_2, w_3, …, w_m)$ ，即 $w$ 属于 $m$ 列的一维矩阵，$b$是一个系数，$sign$ 是判断函数，大于0为正，否则为负。

其中线性可分意味着有许多直线可以将两类数据正确分类，而其中SVM对应着其中间隔最大的一条直线。

>数据线性可分是指对于数据集 $\{(x_i,y_i)\}(i=1~N)$，$\exists (w,b)$ 对于 $\forall i=1~N$ 有若 $y_i=+1$，则 $w^T x_i + b \geq 0$；若 $y=-1$，则 $w^T x_i + b < 0$

那么如何计算 $w$ 和 $b$ 值呢？在回答这个问题之前，我们需要注意，SVM的学习策略是**间隔最大化**，以下介绍函数间隔和几何间隔，通过间隔问题得到最优化问题。

### 7.1.2 函数间隔和几何间隔

直接给出函数间隔(functional margin)的定义如下

**定义 7.2 (函数间隔)** 对于给定的训练数据集 $T$ 和超平面 $(w,b)$，定义超平面 $(w,b)$ 关于样本点 $(x_i, y_i) 的函数间隔为

$$\hat{r_i}=y_i(w \cdot x_i +b) \tag{7.3}$$

定义超平面 $(w,b)$ 关于训练数据集 $T$ 的函数间隔为超平面 $(w,b)$ 关于 $T$ 中所有样本点 $(x_i, y_i)$ 的函数间隔之最小值，即

$$\hat{r}=\min \limits_{i=1,…,N} \hat{r_i} \tag{7.4}$$

其中 $w \cdot x_i$ 表示两个矩阵的内积，函数间隔可以表示分类预测的正确性及确信度。

但是值得注意一点是，如果成比例的改变 $w$ 和 $b$ ，超平面没有发生变化，但是函数间隔却发生变化。因此，我们对分离超平面的法向量 $w$ 加某些约束，如规范化，$||w||=1$，这样间隔是确定的。这时得到的函数间隔也被称作几何间隔(grometric margin)，几何间隔的定义如下所示

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200812162329459.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MTc4OTYy,size_16,color_FFFFFF,t_70#pic_center)

**定义 7.2 (几何间隔)** 对于给定的训练数据集 $T$ 和超平面 $(w,b)$，定义超平面 $(w,b)$ 关于样本点 $(x_i, y_i) 的几何间隔为

$$\hat{r_i}=y_i(\frac{w}{||w||} \cdot x_i +\frac{b}{||w||}) \tag{7.5}$$

定义超平面 $(w,b)$ 关于训练数据集 $T$ 的几何间隔为超平面 $(w,b)$ 关于 $T$ 中所有样本点 $(x_i, y_i)$ 的几何间隔之最小值，即

$$\hat{r}=\min \limits_{i=1,…,N} \hat{r_i} \tag{7.6}$$

其中 $||w|| = \sqrt{w_1^2+w_2^2+…+w_m^2}$ 为 $w$ 的 $L_2$ 范数

>超平面 $(w,b$ 关于样本点 $(x_i, y_i)$ 的几何间隔一般是实例点到超平面的带符号距离(signed distance)，当样本点被超平面正确分类时就是实例点到超平面的距离

由定义 $(7.3)-(7.6)$ 得到函数间隔与几何间隔的关系如下所示：

$$r_i = \frac{\hat{r_i}}{||w||} \tag{7.7}$$

同理有：

$$r=\frac{\hat{r}}{||w||} \tag{7.8}$$

几何间隔与函数间隔的区别如下所示：

* 若 $||w||=1$，那么函数间隔与几何间隔相等
* 如果 $w, b$ 按比例改变(超平面没有改变)，函数间隔按比例改变，但是几何间隔不会变化

后续会利用到第二个特点化简等式

### 7.1.3 间隔最大化

在数据线性可分的假设条件下，可以正确分类的超平面有无数多个，如何找出最优秀的超平面？这里引入一个判定优秀超平面的方法，间隔最大化(margin maximization)。

间隔最大化的直观解释是：
>对训练数据集找到几何间隔最大的超平面意味着以充分大的确信度对训练数据进行分类

也就是说，不仅要分类正确，还要分的足够开. 这样的超平面应该对未知的新实例有很好的分类预测能力.

#### 1. 最大间隔分离超平面

以下是推导如何获取一个几何间隔最大的分离超平面, 这部分的数学公式比较复杂, 本来打算直接截图的, 但是这样既不美观, 也偏离了复习SVM的初衷. 这里以我自己的理解对这部分进行简要的复述.

将求解最大间隔分离超平面的问题表示为一个约束最优化问题:

$$\max_{w, b} \quad r \tag{7.9}$$

$$s.t. \quad y_i \bigg(\frac{w}{||w||} \cdot x_i + \frac{b}{||w||}\bigg)\geq r,\quad i=1,2,...,N \tag{7.10}$$

在约束条件为超平面 $(w, b)$ 关于每个训练样本点的几何间隔至少是 $r$ 的前提下,最大化超平面 $(w,b)$ 关于训练数据集的几何间隔 $r$.

根据式 $(7.8)$ ,将几何间隔代替为函数间隔:这个问题改写为:

$$\max_{w, b} \quad \frac{\hat{r}}{||w||} \tag{7.11}$$

$$s.t. \quad y_i \bigg(\frac{w}{||w||} \cdot x_i + \frac{b}{||w||}\bigg) \geq \frac{\hat{r}}{||w||},\quad i=1,2,...,N \tag{7.12}$$

函数间隔 $\hat{r}$ 的取值不会影响最优化问题的解. 且将 $w, b$ 等比例的改变为 $\lambda w, \lambda b$, 这时函数间隔为 $\lambda \hat{r}$. 函数间隔的改变对上述最优化问题的不等式约束没有影响, 对目标函数的优化也没有影响. 所以简化上述公式, 取 $\hat{r}=1$, 且注意到最大化 $\frac{1}{||w||}$ 和最大化 $\frac{1}{2}||w||^2$ 是等价的,所以得到以下线性可分支持向量机学习的最优化问题

$$\min_{w, b} \quad \frac{1}{2} ||w||^2 \tag{7.13}$$

$$s.t. \quad y_i (w \cdot x_i + b) -1 \geq 0,\quad i=1,2,...,N \tag{7.14}$$

>* 这里 $\max_{w, b}\frac{1}{||w||}$ 与 $\min_{w, b}\frac{1}{2} ||w||^2$ 等价推导过程如下:
最大化$\frac{1}{||w||} \Rightarrow$ 最大化 $||w|| \Rightarrow$ 为后续求导方便, 将 $||w||$ 改写为 $\frac{1}{2}||w||^2$
>* 限制条件从式 $(7.12)$ 到式 $(7.14)$ 是取 $\hat{r} = 1$ 并不等式左右同时乘上 $||w|| \geq 0$

这是一个**凸二次规划(convex quadratic programming)问题**

凸优化问题是指约束最优化问题

$$\min_w \quad f(w) \tag{7.15}$$

$$s.t. \quad g_i(w) \leq 0, \quad i=1,2,...,k \tag{7.16}$$ 

$$h_i(w)=0, \quad i=1,2,...,l \tag{7.17}$$

其中, 目标函数 $f(w)$ 是二次函数且约束函数 $g_i(w)$ 都是 $R^n$ 上的连续可微的凸函数, 约束函数 $h_i(w)$ 是 $R^n$ 上的仿射函数. 

当目标函数 $f(w)$ 是二次函数且约束函数 $g_i(w)$ 是仿射函数(一次函数)时,上述凸优化问题成为凸二次规划问题. 其要么无解, 要么有解且唯一.

>凸二次规划问题的局部极值也是全局极值

凸函数的一般图像如下图所示

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200812185752914.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MTc4OTYy,size_16,color_FFFFFF,t_70#pic_center)

结合式 $(7.13)-(7.14)$ 得到**最大间隔法(maximum margin method)**

#### 2. 最大间隔分离超平面的存在唯一性

线性可分训练数据集的最大间隔分离超平面是**存在且唯一**的

这里给出了最大间隔法求解唯一的理论支持

**定理 7.1 (最大间隔分离超平面的存在唯一性)** 若训练数据集 $T$ 线性可分,则可将训练数据集中的样本点完全正确分开的最大间隔分离超平面存在且唯一.

具体的证明可以参考[1]中p117-p118

#### 3. 支持向量和间隔边界

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200813172606346.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MTc4OTYy,size_16,color_FFFFFF,t_70#pic_center)

* 线性可分情况下, 训练数据集的样本点中与分离超平面距离最近的样本点的实例称为**支持向量(support vector)**

>支持向量是使约束条件 $(7.14)$ 等号成立的点: $y_i(w \cdot x_i +b)=1$
>如图7.3所示, 在 $H_1$ 上和 $H_2$ 上的点就是*支持向量*

* $H_1$ 和 $H_2$ 之间的距离称为**间隔(margin)**, 间隔依赖于超平面的法向量 $w$, 等于 $\frac{2}{||w||}$
* $H_1$ 和 $H_2$ 称为**间隔边界**
* 分离超平面与间隔边界平行且处于中央

>决定分离超平面时只有支持向量起作用, 支持向量起着绝对的作用, 所以称这种分类模型为支持向量机
>支持向量的个数一般很少, 所以支持向量机由很少的重要训练样本确定

### 7.1.4 学习的对偶算法

支持向量机的最优化问题是式 $(7.13)-(7.14)$, 直接求解这个问题是比较复杂的, 可以利用拉格朗日对偶性, 得到该问题的对偶问题(dual problem).

这样做的好处是:

* 对偶问题更容易求解
* 自然引入核函数

构造拉格朗日函数:

$$L(w,b,a)=\frac{1}{2}||w||^2-\sum_{i=1}^N a_i y_i (w \cdot x_i +b) + \sum_{i=1}^N a_i \tag{7.18}$$

其中$a=(a_1,a_2,...,a_N)^T$是拉格朗日乘子向量

原始问题转换为对偶问题的极大极小解，先求 $L(w,b,a)$ 对 $w,b$ 的极小，在求对 $a$ 的极小

$$\max_a \min_{w,b} L(w,b,a)$$

对上式的求解，按照从里到外的顺序求解，首先求解 $\min_{w, b}L(w,b,a)$，再求解 $\min_{w, b}L(w,b,a)$ 对 $a$ 的极大

1. 求 $\min_{w, b}L(w,b,a)$
拉格朗日函数 $L(w,b,a)$ 分别对 $w,b$ 求偏导并令其等于0

$$\nabla_w L(w,b,a)=w-\sum_{i=1}^N a_i y_i x_i =0$$

$$\nabla_b L(w,b,a)=\sum_{i=1}^N a_i y_i = 0$$

得到

$$w=\sum_{i=1}^N a_i y_i x_i \tag{7.19}$$

$$\nabla_b L(w,b,a)=\sum_{i=1}^N a_i y_i = 0\tag{7.20}$$

将式 $(7.19)$ 代入拉格朗日函数 $(7.18)$，并利用式 $(7.20)$得到

$$\begin{aligned}
L(w,b,a)&=\frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N a_i a_j y_i y_j(x_i \cdot x_j) - \sum_{i=1}^N a_i y_i \Bigg( 
\Bigg(\sum_{j=1}^N a_j y_j x_j \Bigg) \cdot x_i +b \Bigg) + \sum_{i=1}^N a_i \\
&=-\frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N a_i a_j y_i y_j(x_i \cdot x_j) \sum_{i=1}^N a_i
\end{aligned}$$

2. 求 $\min_{w, b}L(w,b,a)$ 对 $a$ 的极大，即是对偶问题

$$\max_a -\frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N a_i a_j y_i y_j (x_i \cdot x_j) + \sum_{i=1}^N a_i \tag{7.21}$$

$$s.t. \quad \sum_{i=1}^N a_i y_i =0 $$

$$a_i\geq 0, i=1,2,...N$$

去掉负号，得到以下等价的对偶优化问题，**求极大变为求极小**

$$\min_a \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N a_i a_j y_i y_j (x_i \cdot x_j) - \sum_{i=1}^N a_i \tag{7.22}$$

$$s.t. \quad \sum_{i=1}^N a_i y_i =0 \tag{7.23}$$

$$a_i\geq 0, i=1,2,...N \tag{7.24}$$

在进行下一步, 考虑原始问题和对偶问题的转化的关系

**定理C.2** 考虑原始问题 $(7.15)-(7.16)$ 和对偶问题 $(7.22)-(7.24)$ 。假设 $(7.15)$ 和 $(7.22)$ 是凸函数， 且限制条件 $(7.16)$ 和 $(7.23)$ 是仿射函数，且不等式约束条件是严格成立的，即存在 $w$，对所有 $i$ 有 $c_i(x) < 0$，则存在 $w^*, a^*, \beta^*$，使得 $x^*$ 是原始问题的解成立， $a^*, \beta^*$ 是对偶问题的解，并且

$$p^*=d^*=L(x^*, a^*, \beta^*) \tag{C.20}$$

可知，原始问题满足以上定理，故有 $w^*,a^*,\beta^*$，使 $w^*$ 是原始问题的解， $a^*, \beta^*$ 是对偶问题的解.

原始问题的求解就转化为对偶问题的求解

**定理 7.2** 设 $a^*=(a_1^*, a_2^*,...,a_l^*)^T$ 是对偶最优化问题的解，则存在下标 $j$，使得 $a_j^*>0$，并可按下式求得原始最优化问题的解

$$w^*=\sum_{i=1}^N a_i^*y_i x_i \tag{7.25}$$

$$b^*=y_i-\sum_{i=1}^Na_i^*y_i(x_i \cdot x_j) \tag{7.26}$$



## 参考文献

[1]. 李航. 统计学习方法(第二版) [M ].北京: 清华大学出版社.2019
[2]. 周志华. 机器学习[M].北京: 清华出版社. 2016
[3]. Aurélien Géron.Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow[M]. America:O’Reilly Media, Inc.2019, June.

